import datetimeimport gcimport osimport pandas as pdimport tensorflow as tffrom env import environmentfrom networks import Qnetwork_lr_bayesos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'MEMORY_SIZE = 512BATCH_SIZE = 64  # 32LEARNING_RATE_min = 1e-4LEARNING_RATE_max = 2e-5  # MIN 1E-7, MAX 5E-5NUM_OF_HIDDEN = [64, 128, 128, 64]  # [128, 256, 32]EGREEDY_STEPS = 6e4TEMPERATURE_steps = 8e4EGREEDY_MAX = 0.5EGREEDY_MIN = 0.1num_of_inputs = 28REPLACE_ITER = 100train_opt_dict = {0: 'egreedy', 1: 'boltzman', 2: 'dropout'}train_opt = train_opt_dict[2]cpu = 'cpu:0'EPOCHS = 203   #if EPOCHS == 199:    prioritized = Falseelse:    prioritized = TrueBETA = 0.01     # 0.01 or 0.02"""    Beta is the discount factor,         0.01 corresponds to 0.54 for per minute        0.001 corresponds to 0.94 ()        0.0017 corrsponds t0 0.90"""end_time = 14 * 3600  # 5 hours of simulationoperation_start_time = datetime.time(7, 0, 0)  # in the morning h:m:senv = environment.Env(num_of_carts=2, end_time=end_time, beta=BETA)sess = tf.Session()with tf.variable_scope('dueling'):    dueling_DQN = Qnetwork_lr_bayes.Q_network(num_of_inputs=num_of_inputs,                                           num_of_hidden_units=NUM_OF_HIDDEN,                                           num_of_actions=2,                                           e_greedy_max=EGREEDY_MAX,                                           e_greedy_min=EGREEDY_MIN,                                           learning_rate_max=LEARNING_RATE_max,                                           learning_rate_min=LEARNING_RATE_min,                                           replace_target_iter=REPLACE_ITER,                                           memory_size=MEMORY_SIZE,                                           batch_size=BATCH_SIZE,                                           e_greedy_steps=EGREEDY_STEPS,  # 1e-3,                                           output_graph=False,                                           dueling=True,                                           sess=None,                                           time_diff=0,                                           beta=BETA,                                           env=env,                                           keep_prop=0.9,                                           temperature_steps=TEMPERATURE_steps,                                           cpu=cpu,                                           action_opt=train_opt,                                           prioritized=prioritized, reload=False)sess.run(tf.global_variables_initializer())def train(RLModel, epoch=1):    acc_r = [0]    total_steps = 0    saver = tf.train.Saver(max_to_keep=5)    saver.save(RLModel.sess, "../models/model_{}_{}/trained_{}.ckpt".format(EPOCHS, train_opt, EPOCHS))    frames_csate_reward = []    for epoch_step in range(epoch):        experience = env.reset()        _, _, done = env.step()        # just to avoid pycharm warnings        action_ind = []        action = None        ta = 0        while not done:            waiting_cost = None            observation = env.get_state            _, action, action_ind = RLModel.choose_action(observation)            try:                experience, waiting_cost, done = env.step(action)                ta_prime = env._sim_time                acc_r.append(waiting_cost + acc_r[-1])  # accumulated waiting_cost                if experience and experience[2] is not None:                    # temp = experience.sars_history_agent[env.current_cart.cartID][-1]                    RLModel.store_transitions(experience)                    observation = experience                    total_steps += 1                if total_steps > MEMORY_SIZE:                    # if train_counter == RLModel.batch_size:                    RLModel.learn()                    # train_counter = 0                    # else:                    #     train_counter += 1                    print('EPOCH: {}'.format(epoch_step))            except done:                env.close()        if epoch_step % 10 == 0:            RLModel.print_report()            # saver.save(RLModel.sess, "../models/model_{}_{}/trained_{}.ckpt".format(EPOCHS, train_opt, EPOCHS), global_step=epoch_step)        if epoch_step % 20 == 0:            # RLModel.print_report()            # saver.save(RLModel.sess, "../models/model_{}_{}/trained_{}.ckpt".format(EPOCHS, train_opt, EPOCHS))            saver.save(RLModel.sess, "../models/model_{}_{}/trained_{}.ckpt".format(EPOCHS, train_opt, EPOCHS),                       global_step=epoch_step)        # saver.save(sess, '../models/model1')        # saver.export_meta_graph('../models/model1.meta')        # tf.train.write_graph(tf.Graph(), '../models/', 'saved_model.pb')        # print('EPOCH: {}'.format(epoch_step))        for (j, cart) in env.carts.items():            frames_csate_reward.append(pd.DataFrame(env._cstate_reward_dict[env.carts[j].cartID],                                                    columns=env.cstate_reward_tuple._fields))        # RLModel.writer.close()        # if epoch_step % 40 == 0:        #     save_path = saver.save(sess, "../models/model_{}_{}/trained_{}.ckpt".format(EPOCHS, train_opt, EPOCHS))    cstate_reward_pd = pd.concat(frames_csate_reward)    cstate_reward_pd.to_csv('../tests/logs/cars_cstate_reward_pd_collected_{}_train.csv'.format(env._envID), sep="\t")    saver.save(RLModel.sess, "../models/model_{}_{}/trained_{}.ckpt".format(EPOCHS, train_opt, EPOCHS))    env.close_funcs()    return RLModel.qmean_hist, RLModel.lost_hist, acc_rqval_mean = []cost_dueling = []reward_dueling = []cost_natural = []reward_natural = []qvalmean_temp, cost_dueling_temp, reward_dueling_temp = train(dueling_DQN, epoch=EPOCHS)# qval_mean.extend(qvalmean_temp)# cost_dueling.extend(cost_dueling_temp)# reward_dueling.extend(reward_dueling_temp)# cost_natural_temp, reward_natural_temp = train(natural_DQN, epoch=EPOCHS)# cost_natural.extend(cost_natural_temp)# reward_natural.extend(reward_natural_temp)## fig1 = plt.figure(1)# # plt.plot(np.array(cost_natural), c='r', label='natural')# plt.plot(np.array(cost_dueling), c='b', label='dueling')# plt.title('Training Loss')# plt.legend(loc='best')# plt.ylabel('cost')# plt.xlabel('training steps')# plt.grid()## fig2 = plt.figure(2)# # plt.plot(np.array(reward_natural), c='r', label='natural')# plt.plot(np.array(reward_dueling), c='b', label='dueling')# plt.title('Accumulated Cost')# plt.legend(loc='best')# plt.ylabel('accumulated reward')# plt.xlabel('training steps')# plt.grid()## fig3 = plt.figure(3)# # plt.plot(np.array(reward_natural), c='r', label='natural')# plt.plot(np.array(qval_mean), c='b', label='dueling')# plt.title('Mean Q-Values')# plt.legend(loc='best')# plt.ylabel('mean target q-values')# plt.xlabel('training steps')# plt.grid()## # plt.show()# fig1.savefig('../tests/logs/fig1_{}.png'.format(EPOCHS))# fig2.savefig('../tests/logs/fig2_{}.png'.format(EPOCHS))# fig3.savefig('../tests/logs/fig3_{}.png'.format(EPOCHS))# Simulate Modeldueling_DQN.keep_prob = 1.0# natural_DQN.keep_prob = 1.0# simulate_net(natural_DQN)# simulate_net(dueling_DQN)print('Epsilon Value {};'.format(dueling_DQN.epsilon))print('Learning Rate: {}'.format(dueling_DQN.lr))print('Step Counter {}'.format(dueling_DQN.learning_step_counter))print('Batch Size {}'.format(dueling_DQN.batch_size))print('Temperature {}'.format(dueling_DQN.temperature))# print('Decay_iter {}'.format(dueling_DQN.temperature))dueling_DQN.print_report(EPOCHS)sess.close()gc.collect()